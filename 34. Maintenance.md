### Contexte
Vous avez un cluster Kubernetes avec trois nœuds :  
- `node1`  
- `node2`  
- `node3`  

Supposons que vous devez effectuer une mise à jour logicielle ou matérielle sur `node1`. Vous allez utiliser les commandes suivantes pour gérer cette situation.

---

### Étape 1 : **Cordonner le nœud (`cordon`)**

Vous empêchez Kubernetes de programmer de nouveaux pods sur `node1` pour vous préparer à la maintenance.

- **Commande :**
  ```bash
  kubectl cordon node1
  ```
- **Résultat :**
  - Les pods existants sur `node1` continuent de fonctionner.
  - Kubernetes ne planifie plus de nouveaux pods sur `node1`.

- **Vérification :**
  ```bash
  kubectl get nodes
  ```
  La sortie montre que `node1` est marqué comme **Unschedulable** :
  ```
  NAME     STATUS   ROLES    AGE   VERSION   SCHEDULING
  node1    Ready    <none>   10d   v1.28.0   Unschedulable
  node2    Ready    <none>   10d   v1.28.0   <none>
  node3    Ready    <none>   10d   v1.28.0   <none>
  ```

---

### Étape 2 : **Drainer le nœud (`drain`)**

Vous déplacez les pods existants sur `node1` vers d'autres nœuds du cluster (`node2` et `node3`).

- **Commande :**
  ```bash
  kubectl drain node1 --ignore-daemonsets --delete-emptydir-data
  ```

- **Résultat :**
  - Tous les pods sur `node1` sont déprogrammés et replanifiés sur `node2` et `node3`.
  - Les DaemonSets (comme `kube-proxy` ou `fluentd`) restent sur `node1`, car ils sont ignorés avec `--ignore-daemonsets`.

- **Vérification :**
  Les pods ne sont plus présents sur `node1` :
  ```bash
  kubectl get pods -o wide
  ```

---

### Étape 3 : Effectuer la maintenance
À ce stade, `node1` est vide de pods applicatifs, et aucun nouveau pod ne sera planifié dessus. Vous pouvez maintenant effectuer votre maintenance (par exemple, une mise à jour ou un redémarrage du nœud).

---

### Étape 4 : **Remettre le nœud en service (`uncordon`)**

Après avoir terminé la maintenance, vous rendez `node1` éligible pour recevoir de nouveaux pods.

- **Commande :**
  ```bash
  kubectl uncordon node1
  ```

- **Résultat :**
  - `node1` peut de nouveau accueillir de nouveaux pods.
  - Kubernetes redevient libre de répartir la charge sur tous les nœuds disponibles.

- **Vérification :**
  ```bash
  kubectl get nodes
  ```
  La sortie montre que `node1` est revenu à un état normal :
  ```
  NAME     STATUS   ROLES    AGE   VERSION   SCHEDULING
  node1    Ready    <none>   10d   v1.28.0   <none>
  node2    Ready    <none>   10d   v1.28.0   <none>
  node3    Ready    <none>   10d   v1.28.0   <none>
  ```

---

### Notes importantes :
- Si des pods ne peuvent pas être déplacés (par exemple, des pods avec `emptyDir` ou des volumes locaux), le drain échouera. Dans ce cas, vous devez vérifier les pods problématiques et résoudre les conflits.
  ```bash
  kubectl get pods --all-namespaces --field-selector spec.nodeName=node1
  ```

Avec ces étapes, vous pouvez gérer un nœud en toute sécurité dans un cluster Kubernetes sans interrompre les services.